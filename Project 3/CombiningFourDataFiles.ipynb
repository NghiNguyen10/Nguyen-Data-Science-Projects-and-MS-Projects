{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import getcwd, listdir, chdir\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "\n",
    "chdir(\"../..\")\n",
    "\n",
    "# data_path = getcwd() + \"/data/sas_to_pandas/joins\"\n",
    "\n",
    "# knowledge_wide = pd.read_csv(f\"{data_path}/KNOWLEDGE_WIDE.csv\")\n",
    "\n",
    "# work_activity_wide = pd.read_csv(f\"{data_path}/ACTIVITY_WIDE.csv\")\n",
    "\n",
    "# work_context_wide = pd.read_csv(f\"{data_path}/CONTEXT_WIDE.csv\")\n",
    "\n",
    "# job_zones_2 = pd.read_csv(f\"{data_path}/JOBZONE2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/GoogleDrive-116816172893150527097/My Drive/My_Work/Pathrise/Fellows/Nghi/Pathrise_Nghi_Coding_Tutorials'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from os import getcwd, listdir\n",
    "from src.skillshed_analysis.cleaning_raw_data import knowledge_wide, work_activity_wide, work_context_wide, job_zones_2\n",
    "\n",
    "\n",
    "# Joins\n",
    "frame_order = [knowledge_wide, work_activity_wide, work_context_wide, job_zones_2]\n",
    "\n",
    "## Merge Function\n",
    "def merge_frames(frames: list, cols_to_join=['SOC_Code']):\n",
    "    # Join the DataFrames\n",
    "    result = frames[0]\n",
    "    for i in range(1, len(frames)):\n",
    "        result = result.merge(frames[i], on=['SOC_Code'], how='inner')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Duplicate Filter Function\n",
    "def find_duplicate_cols(df):\n",
    "    # Find Duplicate Columns\n",
    "    duplicate_cols = []\n",
    "\n",
    "    for col in df.columns.tolist():\n",
    "        if '_x' in col:\n",
    "            duplicate_cols.append(col)\n",
    "        elif '__x' in col:\n",
    "            duplicate_cols.append(col)\n",
    "        elif '_y' in col:\n",
    "            duplicate_cols.append(col)\n",
    "        elif '__y' in col:\n",
    "            duplicate_cols.append(col)\n",
    "        else:\n",
    "            continue\n",
    "    return duplicate_cols\n",
    "\n",
    "# Keep the Title column\n",
    "def create_col(df, col: str):\n",
    "\n",
    "    # Find all columns that contain your input col string\n",
    "    cols = [elem for elem in df.columns.tolist() if col in elem]\n",
    "\n",
    "    # Secondary DataFrame\n",
    "    secondary_df = df[cols]\n",
    "\n",
    "    # Rename Columns\n",
    "    secondary_df.columns = [f\"col_{i+1}\" for i in range(secondary_df.shape[1])]\n",
    "\n",
    "    # Generate a Series of Cardinality Values for the Above Columns\n",
    "    cardinality_series = secondary_df.nunique()\n",
    "\n",
    "    # Choose the record that has the maximum value for cardinality\n",
    "    cardinality_record = cardinality_series[cardinality_series == max(cardinality_series.values)]\n",
    "\n",
    "    # Select the col corresponding to the above record\n",
    "    selected_col = cardinality_record.index[0]\n",
    "\n",
    "    # Save the Series from the above col\n",
    "    saved_series = secondary_df[selected_col]\n",
    "\n",
    "    # Remove the cols\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    # Create the new column\n",
    "    df[col] = saved_series.tolist()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Applying the Functions\n",
    "result_df = create_col(merge_frames(frame_order), 'Title')\n",
    "\n",
    "## Reorder the Columns\n",
    "result_cols = ['SOC_Code', 'Title'] + sorted(result_df.columns.tolist()[1:-1])\n",
    "result_df = result_df[result_cols]\n",
    "\n",
    "# Save as File\n",
    "data_path = getcwd() + \"/data/sas_to_pandas/transformed\"\n",
    "# result_df.to_csv(f\"{data_path}/skillshed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 134)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('latest_streamlit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dddc3159d6d6c125705ecb3944194b37f0271216a7ffcae5e6e07e2d2791bb94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
